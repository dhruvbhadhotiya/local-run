#!/usr/bin/env python3
"""
Campus AI Chat Platform - Admin Control Panel
Unified CLI dashboard for all platform management
"""

import os
import sys
import subprocess
import platform
import time
import json
from pathlib import Path
from typing import Optional, Dict, Any

# Constants
PROJECT_ROOT = Path(__file__).parent
ENV_FILE = PROJECT_ROOT / "config.env"  # Using config.env to avoid conflict with .env venv
ENV_EXAMPLE = PROJECT_ROOT / ".env.example"
MODELS_DIR = PROJECT_ROOT / "models"
LOGS_DIR = PROJECT_ROOT / "logs"

# Model catalog
MODEL_CATALOG = {
    "1": {
        "name": "TinyLlama 1.1B Chat (Q4_K_M)",
        "repo_id": "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF",
        "filename": "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
        "size": "668 MB",
        "ram": "4 GB",
        "gated": False
    },
    "2": {
        "name": "Phi-2 2.7B (Q4_K_M)",
        "repo_id": "TheBloke/phi-2-GGUF",
        "filename": "phi-2.Q4_K_M.gguf",
        "size": "1.6 GB",
        "ram": "6 GB",
        "gated": False
    },
    "3": {
        "name": "Mistral 7B Instruct v0.2 (Q4_K_M)",
        "repo_id": "TheBloke/Mistral-7B-Instruct-v0.2-GGUF",
        "filename": "mistral-7b-instruct-v0.2.Q4_K_M.gguf",
        "size": "4.1 GB",
        "ram": "8 GB",
        "gated": False
    },
    "4": {
        "name": "Llama 2 7B Chat (Q4_K_M)",
        "repo_id": "TheBloke/Llama-2-7B-Chat-GGUF",
        "filename": "llama-2-7b-chat.Q4_K_M.gguf",
        "size": "3.8 GB",
        "ram": "8 GB",
        "gated": False
    },
    "5": {
        "name": "CodeLlama 7B Instruct (Q4_K_M)",
        "repo_id": "TheBloke/CodeLlama-7B-Instruct-GGUF",
        "filename": "codellama-7b-instruct.Q4_K_M.gguf",
        "size": "3.8 GB",
        "ram": "8 GB",
        "gated": False
    },
    "custom": {
        "name": "Custom Model (enter details)",
        "repo_id": "",
        "filename": "",
        "size": "varies",
        "ram": "varies",
        "gated": False
    }
}


def clear_screen():
    """Clear terminal screen"""
    os.system('cls' if platform.system() == 'Windows' else 'clear')


def print_header(title: str):
    """Print formatted header"""
    print("\n" + "=" * 60)
    print(f"  {title}")
    print("=" * 60)


def print_menu_header():
    """Print main menu header"""
    clear_screen()
    print("\n" + "=" * 60)
    print("        CAMPUS AI CHAT - ADMIN CONTROL PANEL")
    print("=" * 60)


def pause():
    """Wait for user input"""
    input("\nPress Enter to continue...")


def load_env() -> Dict[str, str]:
    """Load .env file into dictionary"""
    config = {}
    if ENV_FILE.exists():
        with open(ENV_FILE, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    config[key.strip()] = value.strip()
    return config


def save_env(config: Dict[str, str]):
    """Save dictionary to .env file"""
    content = """# Campus AI Chat Platform Configuration
# Generated by Admin Control Panel

# Server Settings
HOST={HOST}
PORT={PORT}
MAX_CONCURRENT_USERS={MAX_CONCURRENT_USERS}

# Model Settings
MODEL_PATH={MODEL_PATH}
USE_GPU={USE_GPU}
MAX_TOKENS={MAX_TOKENS}
TEMPERATURE={TEMPERATURE}
TOP_P={TOP_P}

# Logging
LOG_LEVEL={LOG_LEVEL}
LOG_FILE={LOG_FILE}

# HuggingFace
HF_TOKEN={HF_TOKEN}
HF_MODEL={HF_MODEL}
"""
    
    # Set defaults for missing keys
    defaults = {
        'HOST': '0.0.0.0',
        'PORT': '8080',
        'MAX_CONCURRENT_USERS': '3',
        'MODEL_PATH': 'models/model.gguf',
        'USE_GPU': 'false',
        'MAX_TOKENS': '512',
        'TEMPERATURE': '0.7',
        'TOP_P': '0.9',
        'LOG_LEVEL': 'INFO',
        'LOG_FILE': 'logs/campus_ai.log',
        'HF_TOKEN': '',
        'HF_MODEL': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'
    }
    
    for key, value in defaults.items():
        if key not in config:
            config[key] = value
    
    with open(ENV_FILE, 'w') as f:
        f.write(content.format(**config))
    
    print("[OK] Configuration saved")


def get_installed_models() -> list:
    """Get list of installed model files"""
    if not MODELS_DIR.exists():
        return []
    return list(MODELS_DIR.glob("*.gguf"))


def get_system_info() -> Dict[str, Any]:
    """Get system information"""
    info = {
        "os": platform.system(),
        "python": platform.python_version(),
        "cpu_count": os.cpu_count(),
    }
    
    # RAM
    try:
        import psutil
        ram = psutil.virtual_memory()
        info["ram_total"] = f"{ram.total / (1024**3):.1f} GB"
        info["ram_available"] = f"{ram.available / (1024**3):.1f} GB"
        info["ram_percent"] = f"{ram.percent}%"
    except ImportError:
        info["ram_total"] = "Unknown (install psutil)"
    
    # GPU
    info["gpu"] = detect_gpu()
    
    return info


def detect_gpu() -> str:
    """Detect GPU availability"""
    try:
        import torch
        if torch.cuda.is_available():
            return f"CUDA: {torch.cuda.get_device_name(0)}"
    except ImportError:
        pass
    
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'], 
                              capture_output=True, text=True, timeout=5)
        if result.returncode == 0 and result.stdout.strip():
            return f"NVIDIA: {result.stdout.strip().split(chr(10))[0]}"
    except:
        pass
    
    return "Not detected (CPU mode)"


def check_server_status() -> tuple:
    """Check if server is running"""
    try:
        import requests
        config = load_env()
        port = config.get('PORT', '8080')
        response = requests.get(f"http://localhost:{port}/status", timeout=3)
        if response.status_code == 200:
            return True, response.json()
    except:
        pass
    return False, None


# ============================================================
# MENU: MODEL MANAGEMENT
# ============================================================

def menu_model_management():
    """Model management submenu"""
    while True:
        clear_screen()
        print_header("MODEL MANAGEMENT")
        
        # Show installed models
        installed = get_installed_models()
        config = load_env()
        current_model = config.get('MODEL_PATH', 'Not configured')
        
        print(f"\nCurrent Model: {current_model}")
        print(f"\nInstalled Models ({len(installed)}):")
        if installed:
            for model in installed:
                size_mb = model.stat().st_size / (1024 * 1024)
                active = "<- ACTIVE" if str(model.name) in current_model else ""
                print(f"  - {model.name} ({size_mb:.0f} MB) {active}")
        else:
            print("  No models installed")
        
        print("\n" + "-" * 40)
        print("\n  [1] Download new model")
        print("  [2] Switch active model")
        print("  [3] Download from custom repo (gated)")
        print("  [4] Delete a model")
        print("  [0] Back to main menu")
        
        choice = input("\nSelect option: ").strip()
        
        if choice == "1":
            download_model_menu()
        elif choice == "2":
            switch_model_menu(installed)
        elif choice == "3":
            download_custom_model()
        elif choice == "4":
            delete_model_menu(installed)
        elif choice == "0":
            break


def download_model_menu():
    """Download a model from catalog"""
    clear_screen()
    print_header("DOWNLOAD MODEL")
    
    print("\nAvailable Models:\n")
    for key, model in MODEL_CATALOG.items():
        if key != "custom":
            print(f"  [{key}] {model['name']}")
            print(f"      Size: {model['size']} | RAM Required: {model['ram']}")
            print()
    
    choice = input("Select model (1-5) or 'c' to cancel: ").strip()
    
    if choice.lower() == 'c' or choice not in MODEL_CATALOG:
        return
    
    model = MODEL_CATALOG[choice]
    
    print(f"\nDownloading: {model['name']}")
    print(f"From: {model['repo_id']}")
    print(f"Size: {model['size']}")
    
    confirm = input("\nProceed? (Y/n): ").strip().lower()
    if confirm and confirm != 'y':
        return
    
    try:
        from huggingface_hub import hf_hub_download
        
        MODELS_DIR.mkdir(exist_ok=True)
        
        print("\nDownloading... This may take several minutes.\n")
        
        downloaded = hf_hub_download(
            repo_id=model['repo_id'],
            filename=model['filename'],
            local_dir=MODELS_DIR,
            local_dir_use_symlinks=False
        )
        
        print(f"\n[OK] Download complete: {downloaded}")
        
        # Update config
        update = input("\nSet as active model? (Y/n): ").strip().lower()
        if not update or update == 'y':
            config = load_env()
            config['MODEL_PATH'] = f"models/{model['filename']}"
            save_env(config)
            print("[OK] Model set as active")
        
    except Exception as e:
        print(f"\n[X] Download failed: {e}")
    
    pause()


def download_custom_model():
    """Download from custom/gated repository"""
    clear_screen()
    print_header("DOWNLOAD CUSTOM/GATED MODEL")
    
    print("\nFor gated models (like Llama 2, Gemma), you need:")
    print("  1. HuggingFace account")
    print("  2. Accept model license on HuggingFace")
    print("  3. HuggingFace access token")
    
    print("\n" + "-" * 40)
    
    config = load_env()
    current_token = config.get('HF_TOKEN', '')
    
    if current_token:
        print(f"\nCurrent HF Token: {current_token[:8]}...{current_token[-4:]}")
        change = input("Change token? (y/N): ").strip().lower()
        if change == 'y':
            current_token = input("Enter HuggingFace token: ").strip()
    else:
        current_token = input("\nEnter HuggingFace token: ").strip()
    
    if not current_token:
        print("[X] Token required for gated models")
        pause()
        return
    
    # Save token
    config['HF_TOKEN'] = current_token
    save_env(config)
    
    print("\n" + "-" * 40)
    repo_id = input("\nRepo ID (e.g., meta-llama/Llama-2-7b-chat-hf): ").strip()
    if not repo_id:
        return
    
    filename = input("Filename (e.g., model.gguf): ").strip()
    if not filename:
        return
    
    print(f"\nDownloading: {repo_id}/{filename}")
    
    try:
        from huggingface_hub import hf_hub_download
        
        MODELS_DIR.mkdir(exist_ok=True)
        
        downloaded = hf_hub_download(
            repo_id=repo_id,
            filename=filename,
            local_dir=MODELS_DIR,
            local_dir_use_symlinks=False,
            token=current_token
        )
        
        print(f"\n[OK] Download complete: {downloaded}")
        
        # Update config
        update = input("\nSet as active model? (Y/n): ").strip().lower()
        if not update or update == 'y':
            config = load_env()
            config['MODEL_PATH'] = f"models/{filename}"
            save_env(config)
        
    except Exception as e:
        print(f"\n[X] Download failed: {e}")
        print("\nPossible issues:")
        print("  - Invalid token")
        print("  - Haven't accepted model license")
        print("  - Wrong repo/filename")
    
    pause()


def switch_model_menu(installed: list):
    """Switch active model"""
    if not installed:
        print("\nNo models installed. Download one first.")
        pause()
        return
    
    clear_screen()
    print_header("SWITCH ACTIVE MODEL")
    
    config = load_env()
    current = config.get('MODEL_PATH', '')
    
    print("\nInstalled Models:\n")
    for i, model in enumerate(installed, 1):
        active = "<- CURRENT" if model.name in current else ""
        print(f"  [{i}] {model.name} {active}")
    
    choice = input("\nSelect model number: ").strip()
    
    try:
        idx = int(choice) - 1
        if 0 <= idx < len(installed):
            config['MODEL_PATH'] = f"models/{installed[idx].name}"
            save_env(config)
            print(f"\n[OK] Active model changed to: {installed[idx].name}")
            print("\n[!] Restart server to load new model")
    except:
        print("Invalid selection")
    
    pause()


def delete_model_menu(installed: list):
    """Delete an installed model"""
    if not installed:
        print("\nNo models to delete.")
        pause()
        return
    
    clear_screen()
    print_header("DELETE MODEL")
    
    print("\nInstalled Models:\n")
    for i, model in enumerate(installed, 1):
        size_mb = model.stat().st_size / (1024 * 1024)
        print(f"  [{i}] {model.name} ({size_mb:.0f} MB)")
    
    choice = input("\nSelect model to delete: ").strip()
    
    try:
        idx = int(choice) - 1
        if 0 <= idx < len(installed):
            model = installed[idx]
            confirm = input(f"\nDelete {model.name}? (type 'DELETE' to confirm): ").strip()
            if confirm == 'DELETE':
                model.unlink()
                print(f"\n[OK] Deleted: {model.name}")
            else:
                print("Cancelled")
    except:
        print("Invalid selection")
    
    pause()


# ============================================================
# MENU: CONFIGURATION
# ============================================================

def menu_configuration():
    """Configuration submenu"""
    while True:
        clear_screen()
        print_header("CONFIGURATION")
        
        config = load_env()
        
        print("\nCurrent Settings:")
        print("-" * 40)
        print(f"\n  Server:")
        print(f"    Host:     {config.get('HOST', 'Not set')}")
        print(f"    Port:     {config.get('PORT', 'Not set')}")
        print(f"    Max Users: {config.get('MAX_CONCURRENT_USERS', 'Not set')}")
        print(f"\n  Model:")
        print(f"    Path:     {config.get('MODEL_PATH', 'Not set')}")
        print(f"    GPU:      {config.get('USE_GPU', 'Not set')}")
        print(f"\n  Generation:")
        print(f"    Max Tokens:  {config.get('MAX_TOKENS', 'Not set')}")
        print(f"    Temperature: {config.get('TEMPERATURE', 'Not set')}")
        print(f"    Top P:       {config.get('TOP_P', 'Not set')}")
        print(f"\n  HuggingFace Token: {'[OK] Set' if config.get('HF_TOKEN') else '[X] Not set'}")
        
        print("\n" + "-" * 40)
        print("\n  [1] Configure server")
        print("  [2] Tune model parameters")
        print("  [3] Set HuggingFace token")
        print("  [4] Enable/disable GPU")
        print("  [5] Reset to defaults")
        print("  [0] Back to main menu")
        
        choice = input("\nSelect option: ").strip()
        
        if choice == "1":
            configure_server()
        elif choice == "2":
            tune_parameters()
        elif choice == "3":
            set_hf_token()
        elif choice == "4":
            toggle_gpu()
        elif choice == "5":
            reset_config()
        elif choice == "0":
            break


def configure_server():
    """Configure server settings"""
    clear_screen()
    print_header("SERVER CONFIGURATION")
    
    config = load_env()
    
    print("\nLeave blank to keep current value:\n")
    
    host = input(f"  Host [{config.get('HOST', '0.0.0.0')}]: ").strip()
    port = input(f"  Port [{config.get('PORT', '8080')}]: ").strip()
    max_users = input(f"  Max Users [{config.get('MAX_CONCURRENT_USERS', '3')}]: ").strip()
    
    if host: config['HOST'] = host
    if port: config['PORT'] = port
    if max_users: config['MAX_CONCURRENT_USERS'] = max_users
    
    save_env(config)
    print("\n[OK] Server configuration updated")
    print("[!] Restart server for changes to take effect")
    pause()


def tune_parameters():
    """Tune model generation parameters"""
    clear_screen()
    print_header("TUNE MODEL PARAMETERS")
    
    config = load_env()
    
    print("\n  Model Parameters Guide:")
    print("  -" * 20)
    print("  - Max Tokens: 128-2048 (response length)")
    print("  - Temperature: 0.1-1.0 (creativity, 0.7 = balanced)")
    print("  - Top P: 0.1-1.0 (diversity, 0.9 = recommended)")
    print()
    
    max_tokens = input(f"  Max Tokens [{config.get('MAX_TOKENS', '512')}]: ").strip()
    temperature = input(f"  Temperature [{config.get('TEMPERATURE', '0.7')}]: ").strip()
    top_p = input(f"  Top P [{config.get('TOP_P', '0.9')}]: ").strip()
    
    if max_tokens: config['MAX_TOKENS'] = max_tokens
    if temperature: config['TEMPERATURE'] = temperature
    if top_p: config['TOP_P'] = top_p
    
    save_env(config)
    print("\n[OK] Parameters updated")
    print("[!] Restart server for changes to take effect")
    pause()


def set_hf_token():
    """Set HuggingFace token"""
    clear_screen()
    print_header("HUGGINGFACE TOKEN")
    
    config = load_env()
    current = config.get('HF_TOKEN', '')
    
    print("\nHuggingFace token is required for:")
    print("  - Downloading gated models (Llama, Gemma)")
    print("  - Private repositories")
    print("\nGet token at: https://huggingface.co/settings/tokens\n")
    
    if current:
        print(f"Current token: {current[:8]}...{current[-4:]}")
        print()
    
    token = input("Enter new token (or blank to cancel): ").strip()
    
    if token:
        config['HF_TOKEN'] = token
        save_env(config)
        print("\n[OK] Token saved")
    
    pause()


def toggle_gpu():
    """Enable/disable GPU acceleration"""
    clear_screen()
    print_header("GPU CONFIGURATION")
    
    config = load_env()
    current = config.get('USE_GPU', 'false')
    gpu_info = detect_gpu()
    
    print(f"\nGPU Status: {gpu_info}")
    print(f"Currently: {'Enabled' if current.lower() == 'true' else 'Disabled'}")
    print()
    
    if 'Not detected' in gpu_info:
        print("[!] No GPU detected. CPU mode recommended.")
    
    toggle = input("Toggle GPU? (y/N): ").strip().lower()
    
    if toggle == 'y':
        new_value = 'false' if current.lower() == 'true' else 'true'
        config['USE_GPU'] = new_value
        save_env(config)
        print(f"\n[OK] GPU {'enabled' if new_value == 'true' else 'disabled'}")
        print("[!] Restart server for changes to take effect")
    
    pause()


def reset_config():
    """Reset configuration to defaults"""
    clear_screen()
    print_header("RESET CONFIGURATION")
    
    print("\nThis will reset all settings to defaults.")
    print("Your models will NOT be deleted.")
    
    confirm = input("\nReset? (type 'RESET' to confirm): ").strip()
    
    if confirm == 'RESET':
        config = {
            'HOST': '0.0.0.0',
            'PORT': '8080',
            'MAX_CONCURRENT_USERS': '3',
            'MODEL_PATH': 'models/model.gguf',
            'USE_GPU': 'false',
            'MAX_TOKENS': '512',
            'TEMPERATURE': '0.7',
            'TOP_P': '0.9',
            'LOG_LEVEL': 'INFO',
            'LOG_FILE': 'logs/campus_ai.log',
            'HF_TOKEN': '',
            'HF_MODEL': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'
        }
        save_env(config)
        print("\n[OK] Configuration reset to defaults")
    else:
        print("\nCancelled")
    
    pause()


# ============================================================
# MENU: SERVER CONTROL
# ============================================================

def menu_server_control():
    """Server control submenu"""
    while True:
        clear_screen()
        print_header("SERVER CONTROL")
        
        running, status = check_server_status()
        
        if running:
            print("\n  Status: [ONLINE] RUNNING")
            print(f"  Model Loaded: {status.get('model', {}).get('loaded', False)}")
            print(f"  Users: {status.get('current_users', 0)}/{status.get('max_users', 0)}")
        else:
            print("\n  Status: [OFFLINE] NOT RUNNING")
        
        config = load_env()
        port = config.get('PORT', '8080')
        
        print(f"\n  URL: http://localhost:{port}")
        
        print("\n" + "-" * 40)
        print("\n  [1] Start server")
        print("  [2] Stop server (Ctrl+C in terminal)")
        print("  [3] Restart server")
        print("  [4] Check health")
        print("  [5] View live logs")
        print("  [0] Back to main menu")
        
        choice = input("\nSelect option: ").strip()
        
        if choice == "1":
            start_server()
        elif choice == "2":
            print("\nTo stop server: Press Ctrl+C in the terminal running the server")
            pause()
        elif choice == "3":
            restart_server()
        elif choice == "4":
            run_health_check()
        elif choice == "5":
            view_logs()
        elif choice == "0":
            break


def start_server():
    """Start the server"""
    running, _ = check_server_status()
    if running:
        print("\n[!] Server is already running!")
        pause()
        return
    
    print("\nStarting server...")
    
    if platform.system() == 'Windows':
        # Start in new window on Windows
        subprocess.Popen(['start', 'cmd', '/k', 'python', 'server.py'], 
                        shell=True, cwd=PROJECT_ROOT)
    else:
        # Start in background on Linux/Mac
        subprocess.Popen(['python', 'server.py'],
                        cwd=PROJECT_ROOT,
                        stdout=subprocess.DEVNULL,
                        stderr=subprocess.DEVNULL)
    
    print("Server starting in new window...")
    time.sleep(2)
    
    running, _ = check_server_status()
    if running:
        print("[OK] Server is now running!")
    else:
        print("Server may still be loading model...")
    
    pause()


def restart_server():
    """Restart the server"""
    print("\nTo restart:")
    print("  1. Stop current server (Ctrl+C)")
    print("  2. Run: python server.py")
    print("\nOr use start.bat/start.sh")
    pause()


def run_health_check():
    """Run health check"""
    clear_screen()
    print_header("HEALTH CHECK")
    
    try:
        subprocess.run([sys.executable, 'scripts/health_check.py'], cwd=PROJECT_ROOT)
    except Exception as e:
        print(f"Error: {e}")
    
    pause()


def view_logs():
    """View application logs"""
    clear_screen()
    print_header("APPLICATION LOGS")
    
    log_file = LOGS_DIR / "campus_ai.log"
    
    if not log_file.exists():
        print("\nNo log file found.")
        pause()
        return
    
    print(f"\nLast 30 lines from {log_file}:\n")
    print("-" * 60)
    
    try:
        with open(log_file, 'r') as f:
            lines = f.readlines()
            for line in lines[-30:]:
                print(line.rstrip())
    except Exception as e:
        print(f"Error reading logs: {e}")
    
    pause()


# ============================================================
# MENU: SYSTEM INFO
# ============================================================

def menu_system_info():
    """System information submenu"""
    clear_screen()
    print_header("SYSTEM INFORMATION")
    
    info = get_system_info()
    
    print("\n  System:")
    print(f"    OS:      {info['os']}")
    print(f"    Python:  {info['python']}")
    print(f"    CPUs:    {info['cpu_count']}")
    
    print("\n  Memory:")
    print(f"    Total:     {info.get('ram_total', 'Unknown')}")
    print(f"    Available: {info.get('ram_available', 'Unknown')}")
    print(f"    Used:      {info.get('ram_percent', 'Unknown')}")
    
    print("\n  GPU:")
    print(f"    {info['gpu']}")
    
    print("\n  Models Installed:")
    models = get_installed_models()
    if models:
        for m in models:
            size = m.stat().st_size / (1024 * 1024)
            print(f"    - {m.name} ({size:.0f} MB)")
    else:
        print("    None")
    
    print("\n  Recommendations:")
    ram_gb = float(info.get('ram_total', '0').replace(' GB', '') or 0)
    
    if ram_gb >= 16:
        print("    [OK] Can run Mistral 7B, Llama 2 7B")
    elif ram_gb >= 8:
        print("    [OK] Can run Phi-2, smaller models")
    elif ram_gb >= 4:
        print("    [OK] Can run TinyLlama")
    else:
        print("    [!] Limited RAM - use smallest models")
    
    pause()


# ============================================================
# MAIN MENU
# ============================================================

def main_menu():
    """Main menu loop"""
    while True:
        print_menu_header()
        
        # Quick status
        running, status = check_server_status()
        status_icon = "ONLINE" if running else "OFFLINE"
        print(f"\n  Server: {status_icon} {'Running' if running else 'Stopped'}")
        
        config = load_env()
        model = Path(config.get('MODEL_PATH', '')).name or 'Not configured'
        print(f"  Model:  {model}")
        
        print("\n" + "-" * 40)
        print("\n  [1] Model Management")
        print("  [2] Configuration")
        print("  [3] Server Control")
        print("  [4] System Info")
        print("  [0] Exit")
        
        choice = input("\nSelect option: ").strip()
        
        if choice == "1":
            menu_model_management()
        elif choice == "2":
            menu_configuration()
        elif choice == "3":
            menu_server_control()
        elif choice == "4":
            menu_system_info()
        elif choice == "0":
            print("\nGoodbye!\n")
            break
        else:
            print("Invalid option")
            time.sleep(0.5)


def main():
    """Entry point"""
    try:
        main_menu()
    except KeyboardInterrupt:
        print("\n\nExiting...\n")
        sys.exit(0)
    except Exception as e:
        print(f"\n[X] Error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
