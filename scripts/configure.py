#!/usr/bin/env python3
"""
Configuration Wizard - Interactive setup for Campus AI Chat Platform
Auto-detects system capabilities and generates optimal .env configuration
"""

import os
import sys
from pathlib import Path


def print_header(text):
    """Print formatted header"""
    print("\n" + "="*60)
    print(f"  {text}")
    print("="*60 + "\n")


def detect_gpu():
    """Detect if GPU is available"""
    try:
        import torch
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            return True, gpu_name
    except ImportError:
        pass
    
    # Try alternative detection
    try:
        import subprocess
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            return True, "NVIDIA GPU (detected via nvidia-smi)"
    except:
        pass
    
    return False, None


def get_system_recommendations():
    """Get recommended settings based on system"""
    import psutil
    
    ram_gb = psutil.virtual_memory().total / (1024**3)
    cpu_count = psutil.cpu_count()
    gpu_available, gpu_name = detect_gpu()
    
    # Recommended settings
    if ram_gb >= 16 and gpu_available:
        tier = "high"
        max_users = 5
        max_tokens = 1024
    elif ram_gb >= 8:
        tier = "medium"
        max_users = 3
        max_tokens = 512
    else:
        tier = "low"
        max_users = 2
        max_tokens = 256
    
    return {
        "ram_gb": ram_gb,
        "cpu_count": cpu_count,
        "gpu_available": gpu_available,
        "gpu_name": gpu_name,
        "tier": tier,
        "max_users": max_users,
        "max_tokens": max_tokens
    }


def configure_server():
    """Configure server settings"""
    print("Server Configuration:")
    print("-" * 40)
    
    host = input("  Server host [0.0.0.0]: ").strip() or "0.0.0.0"
    port = input("  Server port [8080]: ").strip() or "8080"
    
    return host, port


def configure_model(system_info):
    """Configure model settings"""
    print("\nModel Configuration:")
    print("-" * 40)
    
    # Check for existing model
    models_dir = Path("models")
    if models_dir.exists():
        gguf_files = list(models_dir.glob("*.gguf"))
        if gguf_files:
            print(f"  Found model: {gguf_files[0].name}")
            model_path = f"models/{gguf_files[0].name}"
        else:
            model_path = input("  Model path [models/model.gguf]: ").strip() or "models/model.gguf"
    else:
        model_path = input("  Model path [models/model.gguf]: ").strip() or "models/model.gguf"
    
    # GPU configuration
    if system_info['gpu_available']:
        print(f"\n  GPU Detected: {system_info['gpu_name']}")
        use_gpu = input("  Enable GPU acceleration? [Y/n]: ").strip().lower()
        use_gpu = "true" if use_gpu != 'n' else "false"
    else:
        print("\n  No GPU detected - using CPU")
        use_gpu = "false"
    
    # Token limits
    print(f"\n  Recommended max tokens: {system_info['max_tokens']}")
    max_tokens = input(f"  Max tokens [{system_info['max_tokens']}]: ").strip() or str(system_info['max_tokens'])
    
    temperature = input("  Temperature [0.7]: ").strip() or "0.7"
    top_p = input("  Top P [0.9]: ").strip() or "0.9"
    
    return model_path, use_gpu, max_tokens, temperature, top_p


def configure_concurrency(system_info):
    """Configure concurrency settings"""
    print("\nConcurrency Configuration:")
    print("-" * 40)
    
    print(f"  Recommended max users: {system_info['max_users']}")
    max_users = input(f"  Max concurrent users [{system_info['max_users']}]: ").strip() or str(system_info['max_users'])
    
    return max_users


def configure_logging():
    """Configure logging settings"""
    print("\nLogging Configuration:")
    print("-" * 40)
    
    log_level = input("  Log level (DEBUG/INFO/WARNING/ERROR) [INFO]: ").strip().upper() or "INFO"
    log_file = input("  Log file path [logs/campus_ai.log]: ").strip() or "logs/campus_ai.log"
    
    return log_level, log_file


def generate_env_file(config):
    """Generate .env file from configuration"""
    env_content = f"""# Campus AI Chat Platform Configuration
# Generated by configuration wizard

# Server Settings
HOST={config['host']}
PORT={config['port']}
MAX_CONCURRENT_USERS={config['max_users']}

# Model Settings
MODEL_PATH={config['model_path']}
USE_GPU={config['use_gpu']}
MAX_TOKENS={config['max_tokens']}
TEMPERATURE={config['temperature']}
TOP_P={config['top_p']}

# Logging
LOG_LEVEL={config['log_level']}
LOG_FILE={config['log_file']}

# HuggingFace (optional - only needed for downloading models)
HF_TOKEN=
HF_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0
"""
    
    env_path = Path(".env")
    
    # Backup existing .env
    if env_path.exists():
        import shutil
        backup_path = env_path.with_suffix('.env.backup')
        shutil.copy(env_path, backup_path)
        print(f"\n  Existing .env backed up to {backup_path}")
    
    # Write new .env
    with open(env_path, 'w') as f:
        f.write(env_content)
    
    print(f"  Created .env file")


def main():
    """Main configuration wizard"""
    print_header("CAMPUS AI CHAT - CONFIGURATION WIZARD")
    
    print("This wizard will help you configure the platform")
    print("based on your system capabilities.\n")
    
    # Detect system
    print("Detecting system capabilities...")
    system_info = get_system_recommendations()
    
    print(f"\nSystem Information:")
    print(f"  RAM: {system_info['ram_gb']:.1f} GB")
    print(f"  CPU Cores: {system_info['cpu_count']}")
    print(f"  GPU: {'✓ ' + system_info['gpu_name'] if system_info['gpu_available'] else '✗ Not detected'}")
    print(f"  Performance Tier: {system_info['tier'].upper()}")
    
    input("\nPress Enter to continue...")
    
    # Configuration steps
    host, port = configure_server()
    model_path, use_gpu, max_tokens, temperature, top_p = configure_model(system_info)
    max_users = configure_concurrency(system_info)
    log_level, log_file = configure_logging()
    
    # Summary
    print_header("CONFIGURATION SUMMARY")
    
    config = {
        'host': host,
        'port': port,
        'max_users': max_users,
        'model_path': model_path,
        'use_gpu': use_gpu,
        'max_tokens': max_tokens,
        'temperature': temperature,
        'top_p': top_p,
        'log_level': log_level,
        'log_file': log_file
    }
    
    print("Server:")
    print(f"  Host: {host}")
    print(f"  Port: {port}")
    print(f"\nModel:")
    print(f"  Path: {model_path}")
    print(f"  GPU: {use_gpu}")
    print(f"  Max Tokens: {max_tokens}")
    print(f"\nConcurrency:")
    print(f"  Max Users: {max_users}")
    print(f"\nLogging:")
    print(f"  Level: {log_level}")
    print(f"  File: {log_file}")
    
    confirm = input("\nSave this configuration? [Y/n]: ").strip().lower()
    if confirm and confirm != 'y':
        print("\nConfiguration cancelled.")
        return 1
    
    # Generate .env
    generate_env_file(config)
    
    print_header("CONFIGURATION COMPLETE!")
    print("✓ Configuration saved to .env")
    print("\nNext steps:")
    print("  1. Review .env file if needed")
    print("  2. Start server: python server.py")
    print("  3. Open browser: http://localhost:8080")
    
    return 0


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print("\n\nConfiguration cancelled.")
        sys.exit(1)
    except Exception as e:
        print(f"\n✗ Configuration error: {e}")
        sys.exit(1)
